"""
PTT çˆ¬èŸ²æ ¸å¿ƒæ¨¡çµ„

æä¾›çˆ¬å– PTT ç‰ˆé¢æ–‡ç« è³‡æ–™çš„æ ¸å¿ƒåŠŸèƒ½
åŒ…å«æ–‡ç« æ‘˜è¦è§£æã€é é¢çˆ¬å–ç­‰åŠŸèƒ½
"""

import collections
import datetime
import json
import os
import random
import time
import urllib

import pandas as pd
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
# ç§»é™¤é€²åº¦ç®¡ç†å™¨å°å…¥ï¼Œç°¡åŒ–ç‚ºç›´æ¥çˆ¬å–


# è‡ªå®šç¾©ä¾‹å¤–é¡åˆ¥
class Error(Exception):
    """æ­¤æ¨¡çµ„æ‹‹å‡ºçš„æ‰€æœ‰ä¾‹å¤–çš„åŸºç¤é¡åˆ¥"""
    pass


class InValidBeautifulSoupTag(Error):
    """å› ç‚ºç„¡æ•ˆçš„ BeautifulSoup æ¨™ç±¤è€Œç„¡æ³•å»ºç«‹ ArticleSummary"""
    pass


class NoGivenURLForPage(Error):
    """å»ºç«‹é é¢æ™‚çµ¦å®šäº† None æˆ–ç©ºç™½çš„ URL"""
    pass


class PageNotFound(Error):
    """ç„¡æ³•é€éçµ¦å®šçš„ URL å–å¾—é é¢"""
    pass


class ArtitcleIsRemoved(Error):
    """ç„¡æ³•å¾ ArticleSummary è®€å–å·²è¢«åˆªé™¤çš„æ–‡ç« """
    pass


# å·¥å…·å‡½æ•¸
def parse_std_url(url):
    """è§£ææ¨™æº–çš„ PTT URL

    Args:
        url (str): PTT æ–‡ç«  URL

    Returns:
        tuple: (bbs_url, board_name, article_id)

    Example:
        >>> parse_std_url('https://www.ptt.cc/bbs/Gossiping/M.1512057611.A.16B.html')
        ('https://www.ptt.cc/bbs', 'Gossiping', 'M.1512057611.A.16B')
    """
    prefix, _, basename = url.rpartition('/')
    basename, _, _ = basename.rpartition('.')
    bbs, _, board = prefix.rpartition('/')
    bbs = bbs[1:]
    return bbs, board, basename


def parse_title(title):
    """è§£ææ–‡ç« æ¨™é¡Œä»¥ç²å–æ›´å¤šè³‡è¨Š

    Args:
        title (str): æ–‡ç« æ¨™é¡Œ

    Returns:
        tuple: (category, is_reply, is_forward)

    Example:
        >>> parse_title('Re: [å•å¦] ç¡è¦ºåˆ°åº•å¯ä¸å¯ä»¥ç©¿è¥ªå­')
        ('å•å¦', True, False)
    """
    _, _, remain = title.partition('[')
    category, _, remain = remain.rpartition(']')
    category = category if category else None
    isreply = True if 'Re:' in title else False
    isforward = True if 'Fw:' in title else False
    return category, isreply, isforward


def parse_username(full_name):
    """è§£æç”¨æˆ¶åç¨±ä»¥ç²å–å…¶ç”¨æˆ¶å¸³è™Ÿå’Œæš±ç¨±

    Args:
        full_name (str): ç”¨æˆ¶å…¨å

    Returns:
        tuple: (user_account, nickname)

    Example:
        >>> parse_username('seabox (æ­é™½ç›’ç›’)')
        ('seabox', 'æ­é™½ç›’ç›’')
    """
    name, nickname = full_name.split(' (')
    nickname = nickname.rstrip(')')
    return name, nickname


# Msg æ˜¯ä¸€å€‹ namedtupleï¼Œç”¨æ–¼æ¨¡å‹åŒ–æ¨æ–‡çš„è³‡è¨Š
Msg = collections.namedtuple('Msg', ['type', 'user', 'content', 'ipdatetime'])


class ArticleSummary:
    """ç”¨æ–¼æ¨¡å‹åŒ–æ–‡ç« è³‡è¨Šçš„é¡åˆ¥ï¼Œè©²è³‡è¨Šä¾†è‡ª ArticleListPage"""

    def __init__(self, title, url, score, date, author, mark, removeinfo):
        # æ¨™é¡Œ
        self.title = title
        self.category, self.isreply, self.isforward = parse_title(title)

        # URL
        self.url = url
        _, self.board, self.aid = parse_std_url(url)

        # å…ƒè³‡æ–™
        self.score = score
        self.date = date
        self.author = author
        self.mark = mark

        # åˆªé™¤è³‡è¨Š
        self.isremoved = True if removeinfo else False
        self.removeinfo = removeinfo

    @classmethod
    def from_bs_tag(cls, tag):
        """å¾å°æ‡‰çš„ bs æ¨™ç±¤å»ºç«‹ ArticleSummary ç‰©ä»¶çš„é¡åˆ¥æ–¹æ³•"""
        try:
            removeinfo = None
            title_tag = tag.find('div', class_='title')
            a_tag = title_tag.find('a')

            if not a_tag:
                removeinfo = title_tag.get_text().strip()

            if not removeinfo:
                title = a_tag.get_text().strip()
                url = a_tag.get('href').strip()
                score = tag.find('div', class_='nrec').get_text().strip()
            else:
                title = 'æœ¬æ–‡ç« å·²è¢«åˆªé™¤'
                url = ''
                score = ''

            date = tag.find('div', class_='date').get_text().strip()
            author = tag.find('div', class_='author').get_text().strip()
            mark = tag.find('div', class_='mark').get_text().strip()
        except Exception:
            raise InValidBeautifulSoupTag(tag)

        return cls(title, url, score, date, author, mark, removeinfo)

    def __repr__(self):
        return '<Summary of Article("{}")>'.format(self.url)

    def __str__(self):
        return self.title

    def read(self):
        """å¾ URL è®€å–æ–‡ç« ä¸¦è¿”å› ArticlePage
        å¦‚æœæ–‡ç« å·²è¢«åˆªé™¤ï¼Œå‰‡å¼•ç™¼ ArtitcleIsRemoved éŒ¯èª¤
        """
        if self.isremoved:
            raise ArtitcleIsRemoved(self.removeinfo)
        return ArticlePage(self.url)


class Page:
    """é é¢çš„åŸºç¤é¡åˆ¥
    é€šé URL ç²å–ç¶²é çš„ HTML å…§å®¹
    æ‰€æœ‰å­é¡åˆ¥çš„ç‰©ä»¶éƒ½æ‡‰è©²å…ˆèª¿ç”¨å®ƒçš„ __init__ æ–¹æ³•
    """
    ptt_domain = 'https://www.ptt.cc'

    def __init__(self, url):
        if not url:
            raise NoGivenURLForPage

        self.url = url

        url = urllib.parse.urljoin(self.ptt_domain, self.url)
        
        # ä½¿ç”¨ fake-useragent å’Œ 1 ç§’è¶…æ™‚
        try:
            from fake_useragent import UserAgent
            ua = UserAgent()
            user_agent = ua.random
        except:
            # å¦‚æœ fake-useragent ç„¡æ³•ä½¿ç”¨ï¼Œä½¿ç”¨é è¨­çš„ User-Agent
            user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        
        resp = requests.get(
            url=url, 
            cookies={'over18': '1'}, 
            verify=True, 
            timeout=1,
            headers={'User-Agent': user_agent}
        )

        if resp.status_code == requests.codes.ok:
            self.html = resp.text
        else:
            raise PageNotFound(f"HTTP {resp.status_code}")


class ArticleListPage(Page):
    """ç”¨æ–¼æ¨¡å‹åŒ–æ–‡ç« åˆ—è¡¨é é¢çš„é¡åˆ¥"""

    def __init__(self, url):
        super().__init__(url)

        # è¨­å®šæ–‡ç« æ¨™ç±¤
        soup = BeautifulSoup(self.html, 'lxml')
        self.article_summary_tags = soup.find_all('div', 'r-ent')
        self.article_summary_tags.reverse()

        # è¨­å®šç›¸é—œ URL
        action_tags = soup.find('div', class_='action-bar').find_all('a')
        self.related_urls = {}
        url_names = 'board man oldest previous next newest'
        for idx, name in enumerate(url_names.split()):
            self.related_urls[name] = action_tags[idx].get('href')

        # è¨­å®šç‰ˆé¢å’Œç´¢å¼•
        _, self.board, basename = parse_std_url(url)
        _, _, idx = basename.partition('index')
        if idx:
            self.idx = int(idx)
        else:
            _, self.board, basename = parse_std_url(
                self.related_urls['previous'])
            _, _, idx = basename.partition('index')
            self.idx = int(idx)+1

    @classmethod
    def from_board(cls, board, index=''):
        """å¾çµ¦å®šçš„ç‰ˆåå’Œç´¢å¼•å»ºç«‹ ArticleListPage ç‰©ä»¶çš„é¡åˆ¥æ–¹æ³•
        å¦‚æœæœªçµ¦å®šç´¢å¼•ï¼Œå‰‡å»ºç«‹ä¸¦è¿”å›è©²ç‰ˆçš„æœ€æ–° ArticleListPage
        """
        url = '/'.join(['/bbs', board, 'index'+str(index)+'.html'])
        return cls(url)

    def __repr__(self):
        return 'ArticleListPage("{}")'.format(self.url)

    def __iter__(self):
        return self.article_summaries

    def get_article_summary(self, index):
        return ArticleSummary.from_bs_tag(self.article_summary_tags[index])

    @property
    def article_summaries(self):
        return (ArticleSummary.from_bs_tag(tag) for tag in self.article_summary_tags)

    @property
    def previous(self):
        return ArticleListPage(self.related_urls['previous'])

    @property
    def next(self):
        return ArticleListPage(self.related_urls['next'])

    @property
    def oldest(self):
        return ArticleListPage(self.related_urls['oldest'])

    @property
    def newest(self):
        return ArticleListPage(self.related_urls['newest'])


class ArticlePage(Page):
    """ç”¨æ–¼æ¨¡å‹åŒ–æ–‡ç« é é¢çš„é¡åˆ¥"""

    default_attrs = ['board', 'aid', 'author', 'date', 'content', 'ip']
    default_csv_attrs = default_attrs + ['pushes.count.score']
    default_json_attrs = default_attrs + \
        ['pushes.count', 'pushes.simple_expression']

    def __init__(self, url):
        super().__init__(url)

        _, _, self.aid = parse_std_url(url)

        # è¨­å®šæ–‡ç« æ¨™ç±¤
        soup = BeautifulSoup(self.html, 'lxml')
        main_tag = soup.find('div', id='main-content')
        meta_value_tags = main_tag.find_all(
            'span', class_='article-meta-value')

        # è™•ç†å…ƒè³‡æ–™
        try:
            self.author = meta_value_tags[0].get_text().strip()
            self.board = meta_value_tags[1].get_text().strip()
            self.title = meta_value_tags[2].get_text().strip()
            self.date = meta_value_tags[3].get_text().strip()

            self.category, self.isreply, self.isforward = parse_title(
                self.title)
            self.datetime = datetime.datetime.strptime(
                self.date, '%a %b %d %H:%M:%S %Y')
        except:
            self.author, self.board, self.title, self.date = '', '', '', ''
            self.category, self.isreply, self.isforward = '', False, False
            self.datetime = None

        # ç§»é™¤å…ƒè³‡æ–™æ¨™ç±¤
        for tag in main_tag.select('div.article-metaline'):
            tag.extract()
        for tag in main_tag.select('div.article-metaline-right'):
            tag.extract()

        # ç²å–æ¨æ–‡ä¸¦ç§»é™¤æ¨æ–‡æ¨™ç±¤
        self.pushes = Pushes(self)
        push_tags = main_tag.find_all('div', class_='push')
        for tag in push_tags:
            tag.extract()
        for tag in push_tags:
            if not tag.find('span', 'push-tag'):
                continue
            push_type = tag.find(
                'span', class_='push-tag').string.strip(' \t\n\r')
            push_user = tag.find(
                'span', class_='push-userid').string.strip(' \t\n\r')
            push_content = tag.find('span', class_='push-content').strings
            push_content = ' '.join(push_content)[1:].strip(' \t\n\r')
            push_ipdatetime = tag.find(
                'span', class_='push-ipdatetime').string.strip(' \t\n\r')
            msg = Msg(type=push_type, user=push_user,
                      content=push_content, ipdatetime=push_ipdatetime)
            self.pushes.addmsg(msg)
        self.pushes.countit()

        # è™•ç†ç‰¹æ®Šé …ç›®
        ip_tags = main_tag.find_all('span', class_='f2')
        dic = {}
        for tag in ip_tags:
            if 'â€»' in tag.get_text():
                key, _, value = tag.get_text().partition(':')
                key = key.strip('â€»').strip()
                value = value.strip()
                if 'å¼•è¿°' in key:
                    continue
                else:
                    dic.setdefault(key, []).append(value)
                    tag.extract()
        
        # å®‰å…¨åœ°æå– IP è³‡è¨Š
        try:
            if 'ç™¼ä¿¡ç«™' in dic and dic['ç™¼ä¿¡ç«™']:
                self.ip = dic['ç™¼ä¿¡ç«™'][0].split()[-1]
            else:
                # å˜—è©¦å…¶ä»–å¯èƒ½çš„æ ¼å¼
                self.ip = self._extract_ip_fallback(main_tag)
        except (IndexError, AttributeError) as e:
            print(f"âš ï¸  ç„¡æ³•æå– IP è³‡è¨Š: {e}")
            self.ip = 'Unknown'

        # ç§»é™¤ richcontent æ¨™ç±¤
        for tag in main_tag.find_all('div', class_='richcontent'):
            tag.extract()

        # è™•ç†è½‰éŒ„è³‡è¨Š
        trans = []
        for tag in main_tag.find_all('span', class_='f2'):
            if 'è½‰éŒ„è‡³çœ‹æ¿' in tag.get_text():
                trans.append(tag.previous_element.parent)
                trans.append(tag.get_text())
                trans.append(tag.next_sibling)
                tag.previous_element.parent.extract()
                tag.next_sibling.extract()
                tag.extract()

        # åˆ†å‰²ä¸»è¦å…§å®¹å’Œç°½åæª”
        try:
            main_content_str = str(main_tag)
            if '--' in main_content_str:
                parts = main_content_str.split('--')
                self.content = parts[0].strip()
                self.signature = parts[1] if len(parts) > 1 else ''
            else:
                # å¦‚æœæ²’æœ‰ç°½åæª”åˆ†éš”ç¬¦è™Ÿï¼Œå°‡æ•´å€‹å…§å®¹è¦–ç‚ºæ­£æ–‡
                self.content = main_content_str.strip()
                self.signature = ''
        except Exception as e:
            print(f"âš ï¸  å…§å®¹åˆ†å‰²éŒ¯èª¤: {e}")
            self.content = str(main_tag).strip()
            self.signature = ''

        # æ¸…ç†å…§å®¹æ ¼å¼
        try:
            contents = self.content.split('\n')
            self.content = '\n'.join(content for content in contents if not (
                '<div' in content and 'main-content' in content))

            if self.signature:
                contents = self.signature.split('\n')
                self.signature = '\n'.join(
                    content for content in contents if not ('</div' in content))
        except Exception as e:
            print(f"âš ï¸  å…§å®¹æ¸…ç†éŒ¯èª¤: {e}")
            # ä¿æŒåŸå§‹å…§å®¹

    @classmethod
    def from_board_aid(cls, board, aid):
        url = '/'.join(['/bbs', board, aid+'.html'])
        return cls(url)

    def __repr__(self):
        return 'ArticlePage("{}")'.format(self.url)

    def __str__(self):
        return self.title

    @classmethod
    def _recur_getattr(cls, obj, attr):
        if not '.' in attr:
            try:
                return getattr(obj, attr)
            except:
                return obj[attr]
        attr1, _, attr2 = attr.partition('.')
        obj = cls._recur_getattr(obj, attr1)
        return cls._recur_getattr(obj, attr2)

    def dump_json(self, *attrs, flat=True):
        """æ ¹æ“šæŒ‡å®šçš„å±¬æ€§è½‰å­˜æ–‡ç« ç‚º JSON å­—ä¸²"""
        data = {}
        if not attrs:
            attrs = self.default_json_attrs
        for attr in attrs:
            data[attr] = self._recur_getattr(self, attr)
        if flat:
            return json.dumps(data, ensure_ascii=False)
        else:
            return json.dumps(data, indent=4, ensure_ascii=False)

    def dump_csv(self, *attrs, delimiter=','):
        """æ ¹æ“šæŒ‡å®šçš„å±¬æ€§è½‰å­˜æ–‡ç« ç‚º CSV å­—ä¸²"""
        cols = []
        if not attrs:
            attrs = self.default_csv_attrs
        for attr in attrs:
            cols.append(self._recur_getattr(self, attr))
        cols = [repr(col) if '\n' in str(col) else str(col) for col in cols]
        return delimiter.join(cols)

    def _extract_ip_fallback(self, main_tag):
        """å‚™ç”¨çš„ IP æå–æ–¹æ³•"""
        try:
            # æ–¹æ³•1: å°‹æ‰¾åŒ…å« IP æ ¼å¼çš„æ–‡å­—
            text_content = main_tag.get_text()
            import re
            ip_pattern = r'\((\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\)'
            ip_match = re.search(ip_pattern, text_content)
            if ip_match:
                return ip_match.group(1)
            
            # æ–¹æ³•2: å°‹æ‰¾å…¶ä»–å¯èƒ½çš„ä¾†æºæ¨™è¨˜
            for tag in main_tag.find_all('span', class_='f2'):
                text = tag.get_text()
                if 'ä¾†è‡ª:' in text or 'From:' in text:
                    ip_match = re.search(ip_pattern, text)
                    if ip_match:
                        return ip_match.group(1)
            
            # æ–¹æ³•3: å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›é è¨­å€¼
            return 'Unknown'
            
        except Exception as e:
            print(f"âš ï¸  IP å‚™ç”¨æå–æ–¹æ³•å¤±æ•—: {e}")
            return 'Unknown'


class Pushes:
    """ç”¨æ–¼æ¨¡å‹åŒ–æ–‡ç« æ‰€æœ‰æ¨æ–‡çš„é¡åˆ¥"""

    def __init__(self, article):
        self.article = article
        self.msgs = []
        self.count = 0

    def __repr__(self):
        return 'Pushes({})'.format(repr(self.article))

    def __str__(self):
        return 'Pushes of Article {}'.format(self.Article)

    def addmsg(self, msg):
        self.msgs.append(msg)

    def countit(self):
        count_types = 'all abs like boo neutral'.split()
        self.count = dict(zip(count_types, [0, 0, 0, 0, 0]))
        for msg in self.msgs:
            if msg.type == 'æ¨':
                self.count['like'] += 1
            elif msg.type == 'å™“':
                self.count['boo'] += 1
            else:
                self.count['neutral'] += 1

        self.count['all'] = self.count['like'] + \
            self.count['boo'] + self.count['neutral']
        self.count['score'] = self.count['like'] - self.count['boo']

    @property
    def simple_expression(self):
        msgs = []
        attrs = ['type', 'user', 'content', 'ipdatetime']
        for msg in self.msgs:
            msgs.append(dict(zip(attrs, list(msg))))
        return msgs


def ptt_crawl(Board_Name, start, page):
    """çˆ¬å–å–®ä¸€é é¢çš„æ–‡ç« è³‡æ–™

    Args:
        Board_Name (str): ç‰ˆé¢åç¨±
        start (int): èµ·å§‹é é¢ç·¨è™Ÿ
        page (int): é é¢åç§»é‡

    Returns:
        pandas.DataFrame: åŒ…å«æ–‡ç« è³‡è¨Šçš„ DataFrame
    """
    Board = ArticleListPage.from_board

    # å»ºç«‹éŒ¯èª¤è¨˜éŒ„ç›®éŒ„
    error_dir = os.path.join('errors', Board_Name)
    os.makedirs(error_dir, exist_ok=True)

    error_count = 0
    success_count = 0

    try:
        # æŠ“è©²æ¿é¦–é çš„æ–‡ç« 
        latest_page = Board(Board_Name, start-page)
        print(f'æ­£åœ¨è™•ç† {Board_Name} ç‰ˆç¬¬ {start-page} é ')
    except Exception as e:
        error_filename = f"{error_dir}/page_error_{start-page}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        print(f'ç„¡æ³•è¼‰å…¥é é¢ {start-page}ï¼ŒéŒ¯èª¤: {e}')

        # å˜—è©¦å„²å­˜éŒ¯èª¤é é¢çš„ HTML
        try:
            url = f'https://www.ptt.cc/bbs/{Board_Name}/index{start-page}.html'
            
            # ä½¿ç”¨ fake-useragent
            try:
                from fake_useragent import UserAgent
                ua = UserAgent()
                user_agent = ua.random
            except:
                user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            
            resp = requests.get(
                url, 
                cookies={'over18': '1'}, 
                timeout=1,
                headers={'User-Agent': user_agent}
            )
            with open(error_filename, 'w', encoding='utf-8') as f:
                f.write(f"<!-- Error occurred while loading page: {e} -->\n")
                f.write(f"<!-- URL: {url} -->\n")
                f.write(f"<!-- Status Code: {resp.status_code} -->\n")
                f.write(resp.text)
            print(f'éŒ¯èª¤é é¢å·²å„²å­˜: {error_filename}')
        except Exception as save_error:
            print(f'ç„¡æ³•å„²å­˜éŒ¯èª¤é é¢: {save_error}')

        return pd.DataFrame()

    # æŠ“å–è³‡æ–™
    ptt_aid = []
    ptt_author = []
    ptt_board = []
    ptt_category = []
    ptt_title = []
    ptt_content = []
    ptt_url = []
    ptt_date = []
    ptt_ip = []
    ptt_all = []
    ptt_boo = []
    ptt_like = []
    ptt_neutral = []
    ptt_score = []
    ptt_comment = []

    for summary in latest_page:  # åªè¦æŠ“æœ€æ–°çš„é é¢
        if summary.isremoved:
            continue

        print(f'æ­£åœ¨æŠ“è³‡æ–™ä¸­...{summary.title[:50]}...')
        time.sleep(1)  # å›ºå®šå»¶é² 1 ç§’

        try:
            article = summary.read()
            # å°‡æ‰€æœ‰å…§å®¹å„²å­˜åœ¨ä¸€å€‹[]
            ptt_aid.append(article.aid)
            ptt_author.append(article.author)
            ptt_board.append(article.board)
            ptt_category.append(article.category)
            ptt_title.append(article.title)
            ptt_content.append(article.content)
            ptt_url.append(article.url)
            ptt_date.append(article.date)
            ptt_ip.append(article.ip)
            ptt_all.append(article.pushes.count['all'])
            ptt_boo.append(article.pushes.count['boo'])
            ptt_like.append(article.pushes.count['like'])
            ptt_neutral.append(article.pushes.count['neutral'])
            ptt_score.append(article.pushes.count['score'])
            ptt_comment.append(article.pushes.simple_expression)

            success_count += 1

        except Exception as e:
            error_count += 1
            article_url = summary.url if hasattr(
                summary, 'url') and summary.url else 'unknown'
            article_title = summary.title if hasattr(
                summary, 'title') and summary.title else 'unknown'

            print(f'è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {article_title[:30]}... - {str(e)[:100]}')

            # å„²å­˜éŒ¯èª¤æ–‡ç« çš„ HTML
            error_filename = f"{error_dir}/article_error_{summary.aid if hasattr(summary, 'aid') else 'unknown'}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.html"

            try:
                if article_url and article_url != 'unknown':
                    full_url = f"https://www.ptt.cc{article_url}"
                    
                    # ä½¿ç”¨ fake-useragent å’Œ 1 ç§’è¶…æ™‚
                    try:
                        from fake_useragent import UserAgent
                        ua = UserAgent()
                        user_agent = ua.random
                    except:
                        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    
                    resp = requests.get(
                        full_url, 
                        cookies={'over18': '1'}, 
                        timeout=1,
                        headers={'User-Agent': user_agent}
                    )

                    with open(error_filename, 'w', encoding='utf-8') as f:
                        f.write(
                            f"<!-- Error occurred while processing article -->\n")
                        f.write(f"<!-- Article Title: {article_title} -->\n")
                        f.write(f"<!-- Article URL: {full_url} -->\n")
                        f.write(f"<!-- Error Message: {str(e)} -->\n")
                        f.write(
                            f"<!-- Timestamp: {datetime.datetime.now().isoformat()} -->\n")
                        f.write(f"<!-- Status Code: {resp.status_code} -->\n")
                        f.write(resp.text)

                    print(f'éŒ¯èª¤æ–‡ç« å·²å„²å­˜: {error_filename}')

            except Exception as save_error:
                print(f'ç„¡æ³•å„²å­˜éŒ¯èª¤æ–‡ç« : {save_error}')

            continue

    # å°‡çµæœåšæˆdf
    dic = {
        'æ–‡ç« ç·¨ç¢¼': ptt_aid,
        'ä½œè€…': ptt_author,
        'ç‰ˆå': ptt_board,
        'åˆ†é¡': ptt_category,
        'æ¨™é¡Œ': ptt_title,
        'å…§æ–‡': ptt_content,
        'æ—¥æœŸ': ptt_date,
        'IPä½ç½®': ptt_ip,
        'ç¸½ç•™è¨€æ•¸': ptt_all,
        'å™“': ptt_boo,
        'æ¨': ptt_like,
        'ä¸­ç«‹': ptt_neutral,
        'æ–‡ç« åˆ†æ•¸ï¼ˆæ­£-è² ï¼‰': ptt_score,
        'æ‰€æœ‰ç•™è¨€': ptt_comment
    }
    final_data = pd.DataFrame(dic)
    # å»é™¤ç©ºç™½çš„æ¨™é¡Œ
    final_data = final_data[final_data['æ¨™é¡Œ'] != '']

    print(f'é é¢è™•ç†å®Œæˆ - æˆåŠŸ: {success_count} ç­†ï¼ŒéŒ¯èª¤: {error_count} ç­†')

    return final_data


def crawl_ptt_page(Board_Name='Drink', start='', page_num=5, crawl_all=False):
    """çˆ¬å– PTT ç‰ˆé¢æŒ‡å®šæ•¸é‡çš„é é¢

    Args:
        Board_Name (str): ç‰ˆé¢åç¨±ï¼Œå›ºå®šç‚º 'Drink'
        start (str): èµ·å§‹é é¢ç·¨è™Ÿï¼Œç©ºå­—ä¸²ä»£è¡¨å¾æœ€æ–°é é¢é–‹å§‹
        page_num (int): è¦çˆ¬å–çš„é é¢æ•¸é‡ï¼Œé è¨­ç‚º 5
        crawl_all (bool): æ˜¯å¦çˆ¬å–æ‰€æœ‰é é¢ï¼Œé è¨­ç‚º False

    Returns:
        pandas.DataFrame: åŒ…å«æ–‡ç« è³‡è¨Šçš„ DataFrame
    """
    if crawl_all:
        print(f'ğŸŒŸ é–‹å§‹çˆ¬å– {Board_Name} ç‰ˆçš„æ‰€æœ‰é é¢')
    else:
        print(f'ğŸŒŸ é–‹å§‹çˆ¬å– {Board_Name} ç‰ˆï¼Œå…± {page_num} é ')
    print('=' * 50)

    t_start = time.time()  # è¨ˆæ™‚é–‹å§‹
    result_list = []
    total_success = 0
    total_errors = 0
    page_errors = 0

    # å»ºç«‹ä¸»è¦éŒ¯èª¤è¨˜éŒ„ç›®éŒ„
    error_dir = os.path.join('errors', Board_Name)
    os.makedirs(error_dir, exist_ok=True)

    # æ±ºå®šèµ·å§‹é é¢
    if start.isdigit():
        start = int(start)
    else:
        try:
            index_url = f'https://www.ptt.cc/bbs/{Board_Name}/index.html'
            index_page = ArticleListPage(index_url)
            previous_url = index_page.previous.url
            start = int(previous_url[previous_url.find(
                'index')+5:previous_url.find('.html')]) + 1
            print(f'è‡ªå‹•åµæ¸¬èµ·å§‹é é¢: {start}')
        except Exception as e:
            print(f'ç„¡æ³•å–å¾—èµ·å§‹é é¢ï¼Œä½¿ç”¨é è¨­å€¼: {e}')
            start = 1

    # çˆ¬å–é é¢
    current_page = start
    pages_crawled = 0

    # å¦‚æœæ˜¯çˆ¬å–æ‰€æœ‰é é¢ï¼Œè¨­å®šä¸€å€‹è¼ƒå¤§çš„ä¸Šé™
    if crawl_all:
        max_pages = start  # å¾æœ€æ–°é é¢çˆ¬åˆ°ç¬¬1é 
        print(f'ğŸ“Š é ä¼°æœ€å¤šçˆ¬å–ç´„ {max_pages} é ')
    else:
        max_pages = page_num

    try:
        while pages_crawled < max_pages:
            try:
                page_index = start - pages_crawled
                
                # å¦‚æœçˆ¬å–æ‰€æœ‰é é¢ï¼Œæª¢æŸ¥æ˜¯å¦å·²åˆ°é”æœ€æ—©é é¢
                if crawl_all and page_index <= 0:
                    print('å·²åˆ°é”æœ€æ—©é é¢ï¼Œçˆ¬å–å®Œæˆ')
                    break
                
                if crawl_all:
                    print(f'\n--- è™•ç†ç¬¬ {pages_crawled+1} é  (é é¢ç·¨è™Ÿ: {page_index}) ---')
                else:
                    print(f'\n--- è™•ç†ç¬¬ {pages_crawled+1}/{page_num} é  (é é¢ç·¨è™Ÿ: {page_index}) ---')

                page_data = ptt_crawl(Board_Name=Board_Name,
                                      start=start, page=pages_crawled)

                if not page_data.empty:
                    result_list.append(page_data)
                    page_success = len(page_data)
                    total_success += page_success
                    print(f'ç¬¬ {pages_crawled+1} é å®Œæˆï¼ŒæˆåŠŸå–å¾— {page_success} ç­†è³‡æ–™')
                else:
                    print(f'ç¬¬ {pages_crawled+1} é ç„¡æœ‰æ•ˆè³‡æ–™')
                    page_errors += 1

                pages_crawled += 1

                # é¡¯ç¤ºé€²åº¦
                if pages_crawled % 10 == 0:
                    print(f"ğŸ“Š å·²å®Œæˆ {pages_crawled} é ")

                # åŠ å…¥å»¶é²é¿å…éåº¦è«‹æ±‚
                delay_time = random.uniform(0.5, 1.5)
                time.sleep(delay_time)

            except KeyboardInterrupt:
                print(f'\nâš ï¸ ç”¨æˆ¶ä¸­æ–·çˆ¬å– (Ctrl+C)')
                print(f"çˆ¬èŸ²å·²åœæ­¢")
                break

            except Exception as e:
                page_errors += 1
                print(f'çˆ¬å–ç¬¬ {pages_crawled+1} é æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}')

                # å„²å­˜é é¢éŒ¯èª¤è³‡è¨Š
                error_log_file = os.path.join(error_dir, 'page_errors.log')
                with open(error_log_file, 'a', encoding='utf-8') as f:
                    f.write(
                        f"{datetime.datetime.now().isoformat()} - Page {pages_crawled+1} (index {page_index}): {str(e)}\n")

                pages_crawled += 1
                continue

    except Exception as critical_error:
        print(f'ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {critical_error}')
        return pd.DataFrame()

    # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™
    if not result_list:
        print('\nâŒ æ²’æœ‰æˆåŠŸçˆ¬å–åˆ°ä»»ä½•è³‡æ–™')
        return pd.DataFrame()

    print(f'\n--- è³‡æ–™åˆä½µè™•ç† ---')

    # åˆä½µæ‰€æœ‰è³‡æ–™
    final_data = pd.concat(result_list, ignore_index=True)
    
    # ç§»é™¤é‡è¤‡è³‡æ–™ (ä½¿ç”¨æ­£ç¢ºçš„ä¸­æ–‡æ¬„ä½åç¨±)
    initial_count = len(final_data)
    final_data = final_data.drop_duplicates(subset=['æ¨™é¡Œ', 'ä½œè€…'], keep='first')
    final_count = len(final_data)
    duplicate_count = initial_count - final_count

    # ä¿å­˜è³‡æ–™
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    main_filename = f'ptt_{Board_Name}_{timestamp}.csv'
    latest_filename = f'ptt_{Board_Name}_latest.csv'
    
    # ç¢ºä¿ data ç›®éŒ„å­˜åœ¨
    os.makedirs('data', exist_ok=True)
    
    main_path = os.path.join('data', main_filename)
    latest_path = os.path.join('data', latest_filename)
    
    final_data.to_csv(main_path, index=False, encoding='utf-8-sig')
    final_data.to_csv(latest_path, index=False, encoding='utf-8-sig')

    t_end = time.time()  # è¨ˆæ™‚çµæŸ
    elapsed_time = int(t_end - t_start)

    # ç”¢ç”Ÿè©³ç´°å ±å‘Š
    print(f'\n{"="*50}')
    print(f'ğŸ‰ çˆ¬å–ä»»å‹™å®Œæˆï¼')
    print(f'ğŸ“Š çµ±è¨ˆå ±å‘Š:')
    print(f'   â””â”€ ç‰ˆé¢: {Board_Name}')
    print(f'   â””â”€ è™•ç†é é¢: {pages_crawled} é ')
    print(f'   â””â”€ é é¢éŒ¯èª¤: {page_errors} é ')
    print(f'   â””â”€ æˆåŠŸæ–‡ç« : {total_success} ç¯‡')
    print(f'   â””â”€ é‡è¤‡ç§»é™¤: {duplicate_count} ç¯‡')
    print(f'   â””â”€ æœ€çµ‚è³‡æ–™: {final_count} ç¯‡')
    print(f'   â””â”€ åŸ·è¡Œæ™‚é–“: {elapsed_time} ç§’')
    print(f'\nğŸ“ æª”æ¡ˆå·²å„²å­˜:')
    print(f'   â””â”€ ä¸»æª”æ¡ˆ: {main_path}')
    print(f'   â””â”€ æœ€æ–°æª”æ¡ˆ: {latest_path}')
    print(f'{"="*50}')

    return final_data
    print(f'   â””â”€ æˆåŠŸæ–‡ç« : {total_success} ç­†')
    print(f'   â””â”€ é‡è¤‡ç§»é™¤: {duplicate_count} ç­†')
    print(f'   â””â”€ æœ€çµ‚è³‡æ–™: {final_count} ç­†')
    print(
        f'   â””â”€ èŠ±è²»æ™‚é–“: {elapsed_time} ç§’ ({elapsed_time//60} åˆ† {elapsed_time % 60} ç§’)')
    if crawl_all:
        print(f'   â””â”€ å¹³å‡é€Ÿåº¦: {final_count/elapsed_time:.2f} ç­†/ç§’')
    print(f'   â””â”€ ä¸»è¦æª”æ¡ˆ: {main_path}')
    print(f'   â””â”€ æœ€æ–°æª”æ¡ˆ: {latest_path}')

    if page_errors > 0:
        print(f'   â””â”€ éŒ¯èª¤è¨˜éŒ„: {error_dir}/')

    print(f'{"="*50}')

    return final_data


def main():
    """ç°¡åŒ–çš„ä¸»ç¨‹å¼ - åªçˆ¬å– Drink ç‰ˆ"""
    print('=== PTT Drink ç‰ˆçˆ¬èŸ²å·¥å…· ===')
    print('å›ºå®šçˆ¬å– Drink ç‰ˆé¢')
    
    board_name = 'Drink'  # å›ºå®šçˆ¬å– Drink ç‰ˆ

    print('è«‹è¼¸å…¥æ‚¨æƒ³å¾ç¬¬å¹¾é é–‹å§‹çˆ¬ï¼ˆç›´æ¥æŒ‰ Enter ä½¿ç”¨æœ€æ–°é é¢ï¼‰ï¼š')
    start_input = input().strip()

    print('è«‹è¼¸å…¥æ‚¨æƒ³çˆ¬å¹¾é ï¼š')
    try:
        page_num = int(input().strip())
        if page_num <= 0:
            print('é é¢æ•¸é‡å¿…é ˆå¤§æ–¼ 0')
            return
    except ValueError:
        print('è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—')
        return

    # åŸ·è¡Œçˆ¬èŸ²
    try:
        crawl_ptt_page(Board_Name=board_name, start=start_input, page_num=page_num)
    except KeyboardInterrupt:
        print('\nä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ')
    except Exception as e:
        print(f'åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}')


if __name__ == '__main__':
    main()
